{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AIS Trajectory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importere biblioteker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\avira\\OneDrive - NTNU\\Master EMIL\\1. semester\\ML\\TDT4173-Gruppe-8\\.venv\\Lib\\site-packages\\pytorch_forecasting\\models\\base_model.py:27: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.callbacks import EarlyStopping, LearningRateMonitor\n",
    "from lightning.pytorch.tuner import Tuner\n",
    "from pytorch_forecasting import TimeSeriesDataSet, TemporalFusionTransformer\n",
    "from pytorch_forecasting import BaseModel, MAE, Baseline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vesselId</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6323f2287abc89c0a9631e57</td>\n",
       "      <td>8656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>61e9f466b937134a3c4c0273</td>\n",
       "      <td>8626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>61e9f464b937134a3c4c025f</td>\n",
       "      <td>7958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>61e9f465b937134a3c4c026d</td>\n",
       "      <td>7928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>61e9f465b937134a3c4c0271</td>\n",
       "      <td>7689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>681</th>\n",
       "      <td>61e9f45eb937134a3c4c0235</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>682</th>\n",
       "      <td>61e9f39ab937134a3c4bfdb9</td>\n",
       "      <td>197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>683</th>\n",
       "      <td>61e9f45cb937134a3c4c022b</td>\n",
       "      <td>196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>684</th>\n",
       "      <td>61e9f42cb937134a3c4c00f9</td>\n",
       "      <td>191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>685</th>\n",
       "      <td>61e9f3c6b937134a3c4bfed5</td>\n",
       "      <td>160</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>686 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     vesselId  count\n",
       "0    6323f2287abc89c0a9631e57   8656\n",
       "1    61e9f466b937134a3c4c0273   8626\n",
       "2    61e9f464b937134a3c4c025f   7958\n",
       "3    61e9f465b937134a3c4c026d   7928\n",
       "4    61e9f465b937134a3c4c0271   7689\n",
       "..                        ...    ...\n",
       "681  61e9f45eb937134a3c4c0235    250\n",
       "682  61e9f39ab937134a3c4bfdb9    197\n",
       "683  61e9f45cb937134a3c4c022b    196\n",
       "684  61e9f42cb937134a3c4c00f9    191\n",
       "685  61e9f3c6b937134a3c4bfed5    160\n",
       "\n",
       "[686 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>cog</th>\n",
       "      <th>sog</th>\n",
       "      <th>rot</th>\n",
       "      <th>heading</th>\n",
       "      <th>navstat</th>\n",
       "      <th>etaRaw</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>vesselId</th>\n",
       "      <th>portId</th>\n",
       "      <th>datetime</th>\n",
       "      <th>time_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-01-01 00:00:25</td>\n",
       "      <td>284.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0</td>\n",
       "      <td>88</td>\n",
       "      <td>0</td>\n",
       "      <td>01-09 23:00</td>\n",
       "      <td>-34.74370</td>\n",
       "      <td>-57.85130</td>\n",
       "      <td>61e9f3a8b937134a3c4bfdf7</td>\n",
       "      <td>61d371c43aeaecc07011a37f</td>\n",
       "      <td>2024-01-01 00:00:25</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-01-01 00:00:36</td>\n",
       "      <td>109.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-6</td>\n",
       "      <td>347</td>\n",
       "      <td>1</td>\n",
       "      <td>12-29 20:00</td>\n",
       "      <td>8.89440</td>\n",
       "      <td>-79.47939</td>\n",
       "      <td>61e9f3d4b937134a3c4bff1f</td>\n",
       "      <td>634c4de270937fc01c3a7689</td>\n",
       "      <td>2024-01-01 00:00:36</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-01-01 00:01:45</td>\n",
       "      <td>111.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0</td>\n",
       "      <td>112</td>\n",
       "      <td>0</td>\n",
       "      <td>01-02 09:00</td>\n",
       "      <td>39.19065</td>\n",
       "      <td>-76.47567</td>\n",
       "      <td>61e9f436b937134a3c4c0131</td>\n",
       "      <td>61d3847bb7b7526e1adf3d19</td>\n",
       "      <td>2024-01-01 00:01:45</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-01-01 00:03:11</td>\n",
       "      <td>96.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>142</td>\n",
       "      <td>1</td>\n",
       "      <td>12-31 20:00</td>\n",
       "      <td>-34.41189</td>\n",
       "      <td>151.02067</td>\n",
       "      <td>61e9f3b4b937134a3c4bfe77</td>\n",
       "      <td>61d36f770a1807568ff9a126</td>\n",
       "      <td>2024-01-01 00:03:11</td>\n",
       "      <td>166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-01-01 00:03:51</td>\n",
       "      <td>214.0</td>\n",
       "      <td>19.7</td>\n",
       "      <td>0</td>\n",
       "      <td>215</td>\n",
       "      <td>0</td>\n",
       "      <td>01-25 12:00</td>\n",
       "      <td>35.88379</td>\n",
       "      <td>-5.91636</td>\n",
       "      <td>61e9f41bb937134a3c4c0087</td>\n",
       "      <td>634c4de270937fc01c3a74f3</td>\n",
       "      <td>2024-01-01 00:03:51</td>\n",
       "      <td>206</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  time    cog   sog  rot  heading  navstat       etaRaw  \\\n",
       "0  2024-01-01 00:00:25  284.0   0.7    0       88        0  01-09 23:00   \n",
       "1  2024-01-01 00:00:36  109.6   0.0   -6      347        1  12-29 20:00   \n",
       "2  2024-01-01 00:01:45  111.0  11.0    0      112        0  01-02 09:00   \n",
       "3  2024-01-01 00:03:11   96.4   0.0    0      142        1  12-31 20:00   \n",
       "4  2024-01-01 00:03:51  214.0  19.7    0      215        0  01-25 12:00   \n",
       "\n",
       "   latitude  longitude                  vesselId                    portId  \\\n",
       "0 -34.74370  -57.85130  61e9f3a8b937134a3c4bfdf7  61d371c43aeaecc07011a37f   \n",
       "1   8.89440  -79.47939  61e9f3d4b937134a3c4bff1f  634c4de270937fc01c3a7689   \n",
       "2  39.19065  -76.47567  61e9f436b937134a3c4c0131  61d3847bb7b7526e1adf3d19   \n",
       "3 -34.41189  151.02067  61e9f3b4b937134a3c4bfe77  61d36f770a1807568ff9a126   \n",
       "4  35.88379   -5.91636  61e9f41bb937134a3c4c0087  634c4de270937fc01c3a74f3   \n",
       "\n",
       "             datetime  time_idx  \n",
       "0 2024-01-01 00:00:25         0  \n",
       "1 2024-01-01 00:00:36        11  \n",
       "2 2024-01-01 00:01:45        80  \n",
       "3 2024-01-01 00:03:11       166  \n",
       "4 2024-01-01 00:03:51       206  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\avira\\OneDrive - NTNU\\Master EMIL\\1. semester\\ML\\TDT4173-Gruppe-8\\.venv\\Lib\\site-packages\\pytorch_forecasting\\data\\timeseries.py:1282: UserWarning: Min encoder length and/or min_prediction_idx and/or min prediction length and/or lags are too large for 1 series/groups which therefore are not present in the dataset index. This means no predictions can be made for those series. First 10 removed groups: [{'__group_id__vesselId': '61e9f3cbb937134a3c4bff09'}]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "ais_evaluations = pd.read_csv('data/ais_test.csv')\n",
    "ais_train = pd.read_csv('data/ais_train.csv', sep='|')\n",
    "\n",
    "# Preprocess data\n",
    "ais_train['datetime'] = pd.to_datetime(ais_train['time'])\n",
    "ais_evaluations['datetime'] = pd.to_datetime(ais_evaluations['time'])\n",
    "\n",
    "\n",
    "# Dropp the vessels that appears less than 100 times \n",
    "vessel_counts = ais_train['vesselId'].value_counts().reset_index()\n",
    "vessel_counts.columns = ['vesselId', 'count']\n",
    "vessel_counts = vessel_counts[vessel_counts['count'] > 100]\n",
    "\n",
    "\n",
    "# print all vessels and their counts\n",
    "display(vessel_counts)\n",
    "\n",
    "\n",
    "# Convert time to integer\n",
    "ais_train = ais_train.sort_values(\"datetime\")\n",
    "ais_train['time_idx'] = (ais_train['datetime'] - ais_train['datetime'].min()).dt.seconds\n",
    "\n",
    "ais_evaluations = ais_evaluations.sort_values(\"datetime\")\n",
    "ais_evaluations['time_idx'] = (ais_evaluations['datetime'] - ais_train['datetime'].min()).dt.seconds\n",
    "\n",
    "display(ais_train.head())\n",
    "\n",
    "# Create TimeSeriesDataSet\n",
    "max_prediction_length = int(ais_train['time_idx'].max() * 0.1)\n",
    "max_encoder_length = int(ais_train['time_idx'].max() * 0.1)\n",
    "training_cutoff = ais_train['time_idx'].max() - max_prediction_length\n",
    "\n",
    "\n",
    "training = TimeSeriesDataSet(\n",
    "    ais_train[lambda x: x.time_idx <= training_cutoff],\n",
    "    time_idx='time_idx',\n",
    "    target=['longitude', 'latitude'],\n",
    "    group_ids=['vesselId'],\n",
    "    min_encoder_length=max_encoder_length//2,\n",
    "    max_encoder_length=max_encoder_length,\n",
    "    min_prediction_length=1,\n",
    "    max_prediction_length=max_prediction_length,\n",
    "    allow_missing_timesteps=True,\n",
    "    time_varying_unknown_reals=[\n",
    "        \"cog\",\n",
    "        \"sog\",\n",
    "        \"rot\",\n",
    "        \"heading\",\n",
    "    ],\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\avira\\OneDrive - NTNU\\Master EMIL\\1. semester\\ML\\TDT4173-Gruppe-8\\.venv\\Lib\\site-packages\\pytorch_forecasting\\data\\timeseries.py:1282: UserWarning: Min encoder length and/or min_prediction_idx and/or min prediction length and/or lags are too large for 1 series/groups which therefore are not present in the dataset index. This means no predictions can be made for those series. First 10 removed groups: [{'__group_id__vesselId': '61e9f3adb937134a3c4bfe37'}]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# create validation and training dataset\n",
    "validation = TimeSeriesDataSet.from_dataset(training, ais_train, min_prediction_idx=training.index.time.max() + 1, stop_randomization=True)\n",
    "batch_size = 32\n",
    "train_dataloader = training.to_dataloader(train=True, batch_size=batch_size, num_workers=2)\n",
    "val_dataloader = validation.to_dataloader(train=False, batch_size=batch_size, num_workers=2)\n",
    "\n",
    "# create dataloaders for model\n",
    "batch_size = 32  # set this between 32 to 128\n",
    "train_dataloader = training.to_dataloader(train=True, batch_size=batch_size, num_workers=0)\n",
    "val_dataloader = validation.to_dataloader(train=False, batch_size=batch_size * 10, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\avira\\AppData\\Local\\Temp\\ipykernel_42348\\3048310298.py:7: UserWarning: In pytorch-forecasting models, on versions 1.1.X, the default optimizer defaults to 'adam', if pytorch_optimizer is not installed, otherwise it defaults to 'ranger' from pytorch_optimizer. From version 1.2.0, the default optimizer will be 'adam' regardless of whether pytorch_optimizer is installed, in order to minimize the number of dependencies in default parameter settings. Users who wish to ensure their code continues using 'ranger' as optimizer should ensure that pytorch_optimizer is installed, and set the optimizer parameter explicitly to 'ranger'.\n",
      "  baseline_predictions = Baseline().predict(val_dataloader, return_y=True)\n",
      "c:\\Users\\avira\\OneDrive - NTNU\\Master EMIL\\1. semester\\ML\\TDT4173-Gruppe-8\\.venv\\Lib\\site-packages\\lightning\\pytorch\\utilities\\parsing.py:208: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "c:\\Users\\avira\\OneDrive - NTNU\\Master EMIL\\1. semester\\ML\\TDT4173-Gruppe-8\\.venv\\Lib\\site-packages\\lightning\\pytorch\\utilities\\parsing.py:208: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\avira\\OneDrive - NTNU\\Master EMIL\\1. semester\\ML\\TDT4173-Gruppe-8\\.venv\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\avira\\OneDrive - NTNU\\Master EMIL\\1. semester\\ML\\TDT4173-Gruppe-8\\.venv\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    }
   ],
   "source": [
    "# create dataloaders for model\n",
    "batch_size = 32  # set this between 32 to 128\n",
    "train_dataloader = training.to_dataloader(train=True, batch_size=batch_size, num_workers=0)\n",
    "val_dataloader = validation.to_dataloader(train=False, batch_size=batch_size*2, num_workers=0)\n",
    "\n",
    "# calculate baseline mean absolute error, i.e. predict next value as the last available value from the history\n",
    "baseline_predictions = Baseline().predict(val_dataloader, return_y=True)\n",
    "\n",
    "# Print the size of predictions and ground truth\n",
    "print(f\"Predictions shape: {baseline_predictions.output.shape}\")\n",
    "print(f\"Ground truth shape: {baseline_predictions.y.shape}\")\n",
    "\n",
    "MAE()(baseline_predictions.output, baseline_predictions.y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure network and trainer\n",
    "pl.seed_everything(42)\n",
    "trainer = pl.Trainer(\n",
    "    accelerator=\"cpu\",\n",
    "    # clipping gradients is a hyperparameter and important to prevent divergance\n",
    "    # of the gradient for recurrent neural networks\n",
    "    gradient_clip_val=0.1,\n",
    ")\n",
    "\n",
    "\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    training,\n",
    "    # not meaningful for finding the learning rate but otherwise very important\n",
    "    learning_rate=0.03,\n",
    "    hidden_size=8,  # most important hyperparameter apart from learning rate\n",
    "    # number of attention heads. Set to up to 4 for large datasets\n",
    "    attention_head_size=1,\n",
    "    dropout=0.1,  # between 0.1 and 0.3 are good values\n",
    "    hidden_continuous_size=8,  # set to <= hidden_size\n",
    "    loss=QuantileLoss(),\n",
    "    optimizer=\"Ranger\"\n",
    "    # reduce learning rate if no improvement in validation loss after x epochs\n",
    "    # reduce_on_plateau_patience=1000,\n",
    ")\n",
    "print(f\"Number of parameters in network: {tft.size()/1e3:.1f}k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find optimal learning rate\n",
    "from lightning.pytorch.tuner import Tuner\n",
    "\n",
    "res = Tuner(trainer).lr_find(\n",
    "    tft,\n",
    "    train_dataloaders=train_dataloader,\n",
    "    val_dataloaders=val_dataloader,\n",
    "    max_lr=10.0,\n",
    "    min_lr=1e-6,\n",
    ")\n",
    "\n",
    "print(f\"suggested learning rate: {res.suggestion()}\")\n",
    "fig = res.plot(show=True, suggest=True)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure network and trainer\n",
    "early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=1e-4, patience=10, verbose=False, mode=\"min\")\n",
    "lr_logger = LearningRateMonitor()  # log the learning rate\n",
    "logger = TensorBoardLogger(\"lightning_logs\")  # logging results to a tensorboard\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=50,\n",
    "    accelerator=\"cpu\",\n",
    "    enable_model_summary=True,\n",
    "    gradient_clip_val=0.1,\n",
    "    limit_train_batches=50,  # coment in for training, running valiation every 30 batches\n",
    "    # fast_dev_run=True,  # comment in to check that networkor dataset has no serious bugs\n",
    "    callbacks=[lr_logger, early_stop_callback],\n",
    "    logger=logger,\n",
    ")\n",
    "\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    training,\n",
    "    learning_rate=0.03,\n",
    "    hidden_size=16,\n",
    "    attention_head_size=2,\n",
    "    dropout=0.1,\n",
    "    hidden_continuous_size=8,\n",
    "    loss=QuantileLoss(),\n",
    "    log_interval=10,  # uncomment for learning rate finder and otherwise, e.g. to 10 for logging every 10 batches\n",
    "    optimizer=\"Ranger\",\n",
    "    reduce_on_plateau_patience=4,\n",
    ")\n",
    "print(f\"Number of parameters in network: {tft.size()/1e3:.1f}k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit network\n",
    "trainer.fit(\n",
    "    tft,\n",
    "    train_dataloaders=train_dataloader,\n",
    "    val_dataloaders=val_dataloader,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "from pytorch_forecasting.models.temporal_fusion_transformer.tuning import optimize_hyperparameters\n",
    "\n",
    "# create study\n",
    "study = optimize_hyperparameters(\n",
    "    train_dataloader,\n",
    "    val_dataloader,\n",
    "    model_path=\"optuna_test\",\n",
    "    n_trials=200,\n",
    "    max_epochs=50,\n",
    "    gradient_clip_val_range=(0.01, 1.0),\n",
    "    hidden_size_range=(8, 128),\n",
    "    hidden_continuous_size_range=(8, 128),\n",
    "    attention_head_size_range=(1, 4),\n",
    "    learning_rate_range=(0.001, 0.1),\n",
    "    dropout_range=(0.1, 0.3),\n",
    "    trainer_kwargs=dict(limit_train_batches=30),\n",
    "    reduce_on_plateau_patience=4,\n",
    "    use_learning_rate_finder=False,  # use Optuna to find ideal learning rate or use in-built learning rate finder\n",
    ")\n",
    "\n",
    "# save study results - also we can resume tuning at a later point in time\n",
    "with open(\"test_study.pkl\", \"wb\") as fout:\n",
    "    pickle.dump(study, fout)\n",
    "\n",
    "# show best hyperparameters\n",
    "print(study.best_trial.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the best model according to the validation loss\n",
    "# (given that we use early stopping, this is not necessarily the last epoch)\n",
    "best_model_path = trainer.checkpoint_callback.best_model_path\n",
    "best_tft = TemporalFusionTransformer.load_from_checkpoint(best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calcualte mean absolute error on validation set\n",
    "predictions = best_tft.predict(val_dataloader, return_y=True, trainer_kwargs=dict(accelerator=\"cpu\"))\n",
    "MAE()(predictions.output, predictions.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw predictions are a dictionary from which all kind of information including quantiles can be extracted\n",
    "raw_predictions = best_tft.predict(val_dataloader, mode=\"raw\", return_x=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(10):  # plot 10 examples\n",
    "    best_tft.plot_prediction(raw_predictions.x, raw_predictions.output, idx=idx, add_loss_to_title=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
